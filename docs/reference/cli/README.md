<!--

NOTE: THIS DOCUMENT IS GENERATED. DO NOT EDIT THIS FILE MANUALLY.

Instead, use `export_cli_reference.py` to update the generator.

-->

# Command-Line Interface

The [Resoto Shell](/concepts/components/shell.md) CLI supports various commands that allow you to access the graph database.

:::tip

You can pipe commands using `|` and chain multiple commands using `;`.

:::

# Commands

| Command                         | Description                                                                         |
| ------------------------------- | ----------------------------------------------------------------------------------- |
| [`aggregate`](#aggregate)       | Aggregate this query by the provided specification                                  |
| [`ancestors`](#ancestors)       | Select all ancestors of this node in the graph.                                     |
| [`chunk`](#chunk)               | Chunk incoming elements in batches.                                                 |
| [`clean`](#clean)               | Mark all incoming database objects for cleaning.                                    |
| [`count`](#count)               | Count incoming elements or sum defined property.                                    |
| [`descendants`](#descendants)   | Select all descendants of this node in the graph.                                   |
| [`dump`](#dump)                 | Dump all properties of incoming objects.                                            |
| [`echo`](#echo)                 | Send the provided message to downstream                                             |
| [`env`](#env)                   | Retrieve the environment and pass it to the output stream.                          |
| [`flatten`](#flatten)           | Take incoming batches of elements and flattens them to a stream of single elements. |
| [`format`](#format)             | Transform incoming objects as string with a defined format.                         |
| [`head`](#head)                 | Return n first elements of the stream.                                              |
| [`http`](#http)                 | Perform http request with incoming data                                             |
| [`jobs`](#jobs)                 | Manage all jobs.                                                                    |
| [`jq`](#jq)                     | Filter and process json.                                                            |
| [`json`](#json)                 | Parse json and pass parsed objects to the output stream.                            |
| [`kind`](#kind)                 | Retrieves information about the graph data kinds.                                   |
| [`list`](#list)                 | Transform incoming objects as string with defined properties.                       |
| [`predecessors`](#predecessors) | Select all predecessors of this node in the graph.                                  |
| [`protect`](#protect)           | Mark all incoming database objects as protected.                                    |
| [`query`](#query)               | Query the graph.                                                                    |
| [`set_desired`](#set_desired)   | Allows to set arbitrary properties as desired for all incoming database objects.    |
| [`set_metadata`](#set_metadata) | Allows to set arbitrary properties as metadata for all incoming database objects.   |
| [`sleep`](#sleep)               | Suspend execution for an interval of time                                           |
| [`start_task`](#start_task)     | Start a task with the given name.                                                   |
| [`successors`](#successors)     | Select all successor of this node in the graph.                                     |
| [`system`](#system)             | Access and manage system wide properties.                                           |
| [`tag`](#tag)                   | Update a tag with provided value or delete a tag                                    |
| [`tail`](#tail)                 | Return n last elements of the stream.                                               |
| [`templates`](#templates)       | Access the query template library.                                                  |
| [`uniq`](#uniq)                 | Remove all duplicated objects from the stream.                                      |
| [`write`](#write)               | Writes the incoming stream of data to a file in the defined format.                 |

## aggregate

**Aggregate this query by the provided specification**

```shell
aggregate [group_prop, .., group_prop]: [function(), .. , function()]
```

This command extends an already existing query. Using the results of a query by aggregating over given properties and applying given aggregation functions.

Aggregate data by using on of the following functions: `sum`, `avg`, `min`, `max` and `count`. Multiple aggregation functions can be applied to the result set by separating them by comma. Each aggregation function can be named via an optional `as &lt;name>` clause.

Aggregation functions can be grouped using aggregation values. Multiple grouping values can be defined by separating them via comma. Each grouping variable can be renamed via an optional `as &lt;name>` clause.

### Parameters

- `group_prop`: the name of the property to use for grouping. Multiple grouping variables are possible, separated by comma. Every grouping variable can be renamed via an as name directive (`prop as prop_name`).
- `function`: grouping function to be applied on every resulting node. Following functions are possible: `sum`, `count`, `min`, `max`, `avg`. The function contains the variable name (e.g.: min(path.to.prop)) It is possible to use static values (e.g.: sum(1)) It is possible to use simple math expressions in the function (e.g. min(path.to.prop \* 3 + 2)) It is possible to name the result of this function (e.g. count(foo) as number_of_foos)

### Examples

```shell
# Count all volumes in the system based on the kind
> query is(volume) | aggregate kind as kind: sum(1) as count
group:
  kind: aws_ec2_volume
count: 1799
---
group:
  kind: gcp_disk
count: 1100

# Count all volumes in the system together with the complete volume size based on the kind
> query is(volume) | aggregate kind: sum(volume_size) as summed, sum(1) as count
group:
  reported.kind: aws_ec2_volume
summed: 130903
count: 1799
---
group:
  reported.kind: gcp_disk
summed: 23930
count: 1100

# Sum the available volume size without any group
> query is(volume) | aggregate sum(volume_size) as summed, sum(1) as count
summed: 154833
count: 2899
```

## ancestors

**Select all ancestors of this node in the graph.**

```shell
ancestors [--with-origin] [edge_type]
```

This command extends an already existing query. It will select all ancestors of the currently selected nodes of the query. The graph may contain different types of edges (e.g. the `default` graph or the `delete` graph). In order to define which graph to walk, the edge_type can be specified.

If --with-origin is specified, the current element is included in the result set as well. Assume node A with descendant B with descendant C: A --> B --> C `query id(C) | ancestors` will select B and A, while `query id(C) | ancestors --with-origin` will select C and B and A.

### Options

- `--with-origin` [Optional, default to false]: includes the current element into the result set.

### Parameters

- `edge_type` [Optional, default to `default`]: Defines the type of edge to navigate.

### Environment Variables

- `edge_type` [Optional]: Defines the type of the edge to navigate. The parameter takes precedence over the env var.

### Examples

```shell
> query is(volume_type) limit 1 | ancestors
kind=gcp_service_sku, id=D2, name=Storage PD Capacity, age=5d8h, cloud=gcp, account=sre
kind=gcp_zone, id=2, name=us-central1-a, age=52yr1mo, cloud=gcp, account=sre, region=us-central1, zone=us-central1-a
kind=gcp_region, id=1000, name=us-central1, age=52yr1mo, cloud=gcp, account=sre, region=us-central1
kind=gcp_service, id=6F81-5844-456A, name=Compute Engine, age=5d8h, cloud=gcp, account=sre
kind=gcp_project, id=sre-tests, name=sre-tests, age=5d8h, cloud=gcp, account=sre
kind=cloud, id=gcp, name=gcp, age=5d8h, cloud=gcp
kind=graph_root, id=root, name=root
```

## chunk

**Chunk incoming elements in batches.**

```shell
chunk [num]
```

Take &lt;num> number of elements from the input stream, put them in a list and send a stream of list downstream. The last chunk might have a lower size than the defined chunk size.

### Parameters

- `num` [optional, defaults to 100] - the number of elements to put into one chunk.

### Examples

```shell
# Chunk an array by putting up to 2 elements into one chunk and sent it downstream.
> json [1,2,3,4,5] | chunk 2
[1, 2]
[3, 4]
[5]

# Chunk an array by putting up to 3 elements into one chunk and sent it downstream.
> json [1,2,3,4,5] | chunk 3
[1, 2, 3]
[4, 5]

# The output of query can be chunked as well. The result is omitted here for brevity.
> query is(volume) limit 5 | chunk 3
```

### Related

- `flatten` - for flattening a chunked input stream.

## clean

**Mark all incoming database objects for cleaning.**

```shell
clean [reason]
```

Mark incoming objects for cleanup. All objects marked as such will eventually be cleaned up in the next delete run.

An optional reason can be provided. This reason is used to log each marked element, which can be useful to understand the reason a resource is cleaned later on.

This command assumes, that all incoming elements are either objects coming from a query or are object ids. All objects coming from a query will have a property `id`.

The result of this command will emit the updated object.

### Parameters

- `reason` [Optional] - a log message is issued with this reason, once a resource is marked for cleanup.

### Examples

```shell
# Query for volumes that have not been accessed in the last month
# Mark them for cleanup and show the id as well as the complete desired section.
> query is(volume) and last_access>1month | clean "Volume not accessed for longer than 1 month" | list id, /desired
id=vol-123, clean=true

# Manually mark a list of resources for cleanup.
> json ["vol-123"] | clean | list id, /desired
id=vol-123, clean=true
```

## count

**Count incoming elements or sum defined property.**

```shell
count [arg]
```

In case no arg is given, it counts the number of instances provided to count. In case of arg: it pulls the property with the name of arg and counts the occurrences of this property.

This command is part of a query. `count` uses an aggregation query under the hood. In case you need more advances aggregations, please see `help aggregation`.

### Parameters

- `arg` [optional]: Instead of counting the instances, count the occurrences of given instance.

### Examples

```shell
# Json array with 3 objects is defined and then counted.
> json [{"a": 1}, {"a": 2}, {"a": 1}] | count
total matched: 3
total unmatched: 0

# Json array with 3 objects is defined. This time the occurrences of the value of a is counted.
> json [{"a": 1}, {"a": 2}, {"a": 1}] | count a
2: 1
1: 2
total matched: 3
total unmatched: 0

> json [{"a": 1}, {"a": 2}, {"a": 3}] | count b
total matched: 0
total unmatched: 3

> query all | count
total matched: 142670
total unmatched: 0

> query all | count /ancestors.cloud.reported.name
gcp: 42403
aws: 93168
total matched: 135571
total unmatched: 0
```

## descendants

**Select all descendants of this node in the graph.**

```shell
descendants [--with-origin] [edge_type]
```

This command extends an already existing query. It will select all descendants of the currently selected nodes of the query. The graph may contain different types of edges (e.g. the `default` graph or the `delete` graph). In order to define which graph to walk, the edge_type can be specified.

If --with-origin is specified, the current element is included in the result set as well. Assume node A with descendant B with descendant C: A --> B --> C `query id(A) | descendants` will select B and A, while `query id(A) | descendants --with-origin` will select C and B and A.

### Options

- `--with-origin` [Optional, default to false]: includes the current element into the result set.

### Parameters

- `edge_type` [Optional, default to `default`]: Defines the type of edge to navigate.

### Environment Variables

- `edge_type` [Optional]: Defines the type of the edge to navigate. The parameter takes precedence over the env var.

### Examples

```shell
> query is(volume_type) limit 1 | descendants --with-origin
kind=gcp_disk_type, name=pd-standard, age=52yr1mo, cloud=gcp, account=sre, region=us-central1, zone=us-central1-a
kind=gcp_disk, id=881, name=disk-1, age=1yr2mo, cloud=gcp, account=sre, region=us-central1, zone=us-central1-a
```

## dump

**Dump all properties of incoming objects.**

```
dump
```

Dump all properties of an incoming element.

### Example

```shell
> query is(volume) limit 1 | dump
id: 0QcwZ5DHsS58A1tHEk5JRQ
reported:
  kind: gcp_disk
  id: '7027640035137'
  tags:
    owner: 'dev-rel'
  name: gke-cluster-1
  ctime: '2021-08-04T08:31:42Z'
  volume_size: 50
  volume_type: pd-standard
  volume_status: available
  snapshot_before_delete: false
  link: https://www.googleapis.com/compute/v1/projects/eng-ksphere-platform/zones/us-central1-c/disks/gke-cluster-1
  label_fingerprint: nT7_dAxskBs=
  last_attach_timestamp: '2021-08-04T08:31:42Z'
  last_detach_timestamp: '2021-08-04T08:31:42Z'
  age: 5mo25d
metadata:
  protected: false
ancestors:
  cloud:
    reported:
      name: gcp
      id: gcp
  account:
    reported:
      name: eng-ksphere-platform
      id: eng-ksphere-platform
  region:
    reported:
      name: us-central1
      id: '1000'
  zone:
    reported:
      name: us-central1-c
      id: '2002'
```

### Related

- `format` - Create a string from object based on a defined format.
- `list` - Define a list of properties to show.
- `jq` - Define a transformation via the well known `jq` command.

## echo

**Send the provided message to downstream**

```shell
echo &lt;message>
```

Send the provided message to downstream.

### Parameters

- `message` is the message to send downstream.

### Examples

```shell
# Hello World in resoto
> echo Hello World
Hello World

# Echo the current time. The placeholder @TIME@ is replaced during execution time.
> echo The current time is @TIME@
The current time is 09:16:18
```

## env

**Retrieve the environment and pass it to the output stream.**

```shell
env
```

Emits the provided environment. This is useful to inspect the environment given to the CLI interpreter.

### Examples

```shell
# The resotoshell will set the graph, section and a session id.
> env
graph: resoto
section: reported
resoto_session_id: SHQF9MBUEJ

# Environment variables can be defined directly on the command line
> section=desired foo=bla env
graph: resoto
section: desired
resoto_session_id: SHQF9MBUEJ
foo: bla
```

## flatten

**Take incoming batches of elements and flattens them to a stream of single elements.**

```shell
flatten
```

Take array elements from the input stream and put them to the output stream one after the other, while preserving the original order.

### Examples:

```shell
# In case elements of the stream are arrays, they will be flattened.
> json [[1, 2], 3, [4, 5]] | flatten
1
2
3
4
5

# An already flat stream of elements is not changed.
> json [1, 2, 3, 4, 5] | flatten
1
2
3
4
5
```

### Related

- `chunk` to put incoming elements into chunks

## format

**Transform incoming objects as string with a defined format.**

```
format [--json][--ndjson][--text][--cytoscape][--graphml][--dot] [format string]
```

This command creates a string from the json input based on the format string. The format string might contain placeholders in curly braces that access properties of the json object. If a property is not available, it will result in the string `null`. You can either use a format string or you can use a predefined format.

### Options

- `--json` [Optional] - will create a json string from the incoming json. The result will be a json array.
- `--ndjson` [Optional] - will create a json object for every element, where one element fits on one line.
- `--text` [Optional] - will create a text representation of every element.
- `--cytoscape` [Optional] - will create a string representation in the well known cytoscape format. See: [https://js.cytoscape.org/#notation/elements-json](https://js.cytoscape.org/#notation/elements-json)
- `--graphml` [Optional] - will create string representaion of the result in graphml format. See: [http://graphml.graphdrawing.org](http://graphml.graphdrawing.org)
- `--dot` [Optional] - will create a string representation in graphviz dot format. See: [https://graphviz.org/doc/info/lang.html](https://graphviz.org/doc/info/lang.html)

### Parameters

- `format_string` [optional]: a string with any content with placeholders to be filled by the object. Placeholders are defined in curly braces.

### Examples

```shell
# Example json to extract a formatted string using placeholder format
> json {"a":"b", "b": {"c":"d"}} | format >{a}&lt; and not >{b.c}&lt;
>b&lt; and not >d&lt;

# Accessing any nested or list property is possible
> json {"b": {"c":[0,1,2,3]}} | format only select >{b.c[2]}&lt;
only select >2&lt;

> query all | format --json | write out.json
Received a file out.json, which is stored to ./out.json.
```

## head

**Return n first elements of the stream.**

```shell
head [-num]
```

Take [num] number of elements from the input stream and send them downstream. The rest of the stream is discarded.

Note: using a query, the same result can be achieved using `sort` and `limit`.

### Options

- `-num` [optional, defaults to 100]: the number of elements to take from the head.

### Examples

```shell
# Json array with 5 elements is defined. We only take the first 2 elements.
> json [1,2,3,4,5] | head -2
1
2

# A query is performed to select all volumes. Only the first 2 results are taken.
> query is(volume) | head -2
kind=gcp_disk, id=12, name=gke-1, age=5mo26d, cloud=gcp, account=eng, region=us-central1, zone=us-central1-c
kind=gcp_disk, id=34, name=pvc-2, age=4mo16d, cloud=gcp, account=dev, region=us-west1, zone=us-west1-a
```

### Related

- `tail` - take the last number of elements.

## http

**Perform http request with incoming data**

```shell
http[s] [--compress] [--timeout &lt;seconds>] [--no-ssl-verify] [--no-body] [--nr-of-retries &lt;num>]
        [http_method] &lt;url> [headers] [query_params]
```

This command takes every object from the incoming stream and sends this object to the defined http(s) endpoint. The payload of the request contains the object. The shape and format of the object can be adjusted with other commands like: list, format, jq, etc. Note: you can use the chunk command to send chunks of objects. E.g.: query is(volume) limit 30 | chunk 10 | http test.foo.org will perform up to 3 requests, where every request will contain up to 10 elements.

### Options

- `--compress` [optional]: enable compression of the request body
- `--timeout` &lt;seconds> [optional, default: 30]: if the request takes longer than the specified seconds it will be aborted
- `--no-ssl-verify` [optional]: the ssl certificate will not be verified.
- `--no-body` [optional]: if this flag is enabled, no content is sent in the request body
- `--nr-of-retries` [optional, default=3]: in case the request is not successful (no 2xx), the request is retried this often. There will be an exponential backoff between the retries.

### Parameters

- `http_method` [optional, default: POST]: one of GET, PUT, POST, DELETE or PATCH
- `url`: the full url of the endpoint to call. Example: https://localhost:8080/call/me If the scheme is not defined, it is taken from the command (http or https). If the host is localhost, it can be omitted (e.g. :8080/call/me)
- `headers`: a list of http headers can be defined via &lt;header_name>:&lt;header_value> Example: HeaderA:test HeaderB:rest Note: You can use quotes to use whitespace chars: "HeaderC:this is the value"
- `query_params`: a list of query parameters can be defined via &lt;param>==&lt;param_value>. Example: param1==test param2==rest Note: You can use quotes to use whitespace chars: "param3==this is the value"

### Examples

```shell
# Look for unencrypted volumes and report them to the specified endpoint
> query is(volume) and reported.volume_encrypted==false | https my.node.org/handle_unencrypted
3 requests with status 200 sent.

# Query all volumes and send chunks of 50 volumes per request to the specified handler
> query is(volume) | chunk 50 | https --compress my.node.org/handle
2 requests with status 200 sent.

# Same query as before, but define special header values and query parameter
> query is(volume) | chunk 50 | https my.node.org/handle "greeting:hello from resotocore" type==volume
2 requests with status 200 sent.
```

## jobs

**Manage all jobs.**

```shell
jobs list
jobs show &lt;id>
jobs add [--id &lt;id>] [--schedule &lt;cron_expression>] [--wait-for-event &lt;event_name>] &lt;command_line>
jobs update &lt;id> [--schedule &lt;cron_expression>] [--wait-for-event &lt;event_name> :] &lt;command_line>
jobs delete &lt;id>
jobs activate &lt;id>
jobs deactivate &lt;id>
jobs run &lt;id>
jobs running
```

- `jobs list`: get the list of all jobs in the system
- `jobs show &lt;id>`: show the current definition of the job defined by given job identifier.
- `jobs add ...`: add a job to the task handler with provided identifier, trigger and command line to execute.
- `jobs update &lt;id> ...` : update trigger and or command line of an existing job with provided identifier.
- `jobs delete &lt;id>`: delete the job with the provided identifier.
- `jobs activate &lt;id>`: activate the triggers of a job.
- `jobs deactivate &lt;id>`: deactivate the triggers of a job. The job will not get started in case the trigger fires.
- `jobs run &lt;id>`: run the job as if the trigger would be triggered.
- `jobs running`: show all currently running jobs.

A job can be scheduled, react on events or both:

- scheduled via a defined cron expression
- event triggered via defined identifier of event to trigger this job
- combined scheduled + event trigger once the schedule triggers this job, it is possible to wait for an incoming event, before the command line is executed.

_Note:_

If a job is triggered, while it is already running, the invocation will wait for the current run to finish. This means that there will be no parallel execution of jobs with the same identifier at any moment in time. A command line is not allowed to run longer than the specified timeout. It is killed in case this timeout is exceeded.

### Options

- `--id` &lt;id> [optional]: The identifier of this job. If no id is defined a random identifier is generated.
- `--schedule` &lt;cron_expression> [optional]: defines the recurrent schedule in crontab format.
- `--wait-for-event` &lt;event_name> [optional]: if defined, the job waits for the specified event to occur. If this parameter is defined in combination with a schedule, the schedule has to trigger first, before the event will trigger the execution.
- `--timeout` [optional, default=3600] Number of seconds, the job is allowed to run. In case this timeout is exceeded, the job run will be killed.

### Parameters

- `command_line` [mandatory]: the CLI command line that will be executed, when the job is triggered. Note: It is recommended to wrap the command line into single quotes or escape all CLI terms like pipe or semicolon (| -> \|). Multiple command lines can be defined by separating them via semicolon.

### Examples

```shell
# print hello world every minute to the console
> jobs add --id say-hello --schedule "* * * * *" echo hello world
Job say-hello added.

# print all available jobs in the system
> jobs list
id: say-hello
trigger:
  cron_expression: '* * * * *'
command: echo hello world

# show a specific job by identifier
> jobs show say-hello
id: say-hello
trigger:
  cron_expression: '* * * * *'
command: echo hello world

# every morning at 4: wait for message of type collect_done and print a message
> jobs add --id early_hi --schedule "0 4 * * *" --wait-for-event collect_done 'match is("volume") | format id'
Job early_hi added.

# wait for message of type collect_done and print a message
> jobs add --id wait_for_collect_done collect_done: echo hello world
Job wait_for_collect_done added.

# run the job directly without waiting for a trigger
> jobs run say-hello
Job say-hello started with id a4bb64cc-7385-11ec-b2cb-dad780437c53.

# show all currently running jobs
> jobs running
job: say-hello
started_at: '2022-01-12T09:01:34Z'
task-id: a4bb64cc-7385-11ec-b2cb-dad780437c53

# triggers can be activated and deactivated.
# Deactivated triggers will not trigger the job.
# The active flag shows the state of activation.
> jobs deactivate say-hello
id: say-hello
command: echo hello world
active: false
trigger:
  cron_expression: '* * * * *'

# activate the triggers of the job.
> jobs activate say-hello
id: say-hello
command: echo hello world
active: true
trigger:
  cron_expression: '* * * * *'

# delete a job
> jobs delete say-hello
Job say-hello deleted.
```

## jq

**Filter and process json.**

```
jq &lt;filter>
```

Use the well known jq JSON processor to manipulate incoming json. Every element from the incoming stream is passed to jq. See: https://stedolan.github.io/jq/ for a list of possible jq filter definitions.

### Parameter

- `filter` the filter definition to create a jq program.

### Examples

```shell
# Query ec2 instances and extract only the name property
> query is(aws_ec2_instance) limit 2| jq .name
build-node-1
prod-23

# Query ec2 instances and create a new json object for each entry with name and owner.
> query is(aws_ec2_instance) limit 2 | jq {name: .name, owner: .tags.owner}
name: build-node-1
owner: frosty
---
name: prod-23
owner: bog-team
```

### Related

- `format` - to format incoming objects to a defined string.
- `list` - create list output for every element.

## json

**Parse json and pass parsed objects to the output stream.**

```shell
json &lt;json-string>
```

The defined json-string will be parsed into a json structure. If the defined element is a json array, each element of the array will be sent downstream. Any other json type will be sent as is.

### Parameters

- `json-string` the json string that is parsed as json.

### Examples

```shell
# A simple json string is parsed.
> json "test"
test

# A json object is parsed.
> json {"a": 1, "b": 2}
a: 1
b: 2

# An array of json objects is parsed. Each element is sent downstream.
> json [{"a":1, "b": 2}, {"c": 3, "d": 4}]
a: 1
b: 2
---
c: 3
d: 4
```

## kind

**Retrieves information about the graph data kinds.**

```shell
kind [-p property_path] [name]
```

kind gives information about the available graph data kinds.

### Options

- `-p` [Optional] property_path: lookup the kind for the defined property path. This will do a reverse lookup and search all kinds for the specified property path.

### Parameters

- `name` [Optional]: show available information about the kind with provided name.

### Examples

```shell

# Show all available kinds.
> kind
access_key
.
.
zone

# Show details about a specific kind.
> kind graph_root
name: graph_root
bases:
- graph_root
properties:
- description: The name of this node.
  kind: string
  name: name
  required: false
- description: All attached tags of this node.
  kind: dictionary[string, string]
  name: tags
  required: false

# Lookup the type of the given property path in the model.
> kind -p reported.tags.owner
name: string
runtime_kind: string
```

## list

**Transform incoming objects as string with defined properties.**

```
list [property [as &lt;name>]] [,property ...]
```

This command creates a string from the json input based on the defined properties to show.

If no prop is defined a predefined list of properties will be shown:

- /reported.kind as kind
- /reported.id as id
- /reported.name as name
- /reported.age as age
- /reported.last_update as last_update
- /ancestors.cloud.reported.name as cloud
- /ancestors.account.reported.name as account
- /ancestors.region.reported.name as region
- /ancestors.zone.reported.name as zone

If property is defined, it will override the default and will show the defined properties. The syntax for property is a comma delimited list of property paths. The property path can be absolute, meaning it with the section name (reported, desired, metadata). In case the section name is not defined, the reported section is assumed automatically.

The defined property path will be looked for every element in the incoming json. If the value is defined, it will be part of the list line. Undefined values are filtered out and will not be printed.

The property name can be defined via an `as` clause. `reported.kind as kind` would look up the path reported.kind and if the value is defined write kind={value} If no as clause is defined, the name of the last element of property path is taken. In the example above we could write `reported.kind` or `reported.kind as kind` - both would end in the same result. The `as` clause is important, in case the last part of the property path is not sufficient as property name.

### Parameters

- property [optional]: a comma separated list of properties to show. Each property defines the path to the property with an optional name.

  _Example_: `path.to.property as prop1`.

### Examples

```shell
# If all parameters are omitted, the predefined default list is taken.
> query is(aws_ec2_instance) limit 3 | list
kind=aws_ec2_instance, id=1, name=sun, ctime=2020-09-10T13:24:45Z, cloud=aws, account=prod, region=us-west-2
kind=aws_ec2_instance, id=2, name=moon, ctime=2021-09-21T01:08:11Z, cloud=aws, account=dev, region=us-west-2
kind=aws_ec2_instance, id=3, name=star, ctime=2021-09-25T23:28:40Z, cloud=aws, account=int, region=us-east-1

# Explicitly define the properties to show without renaming them.
> query is(aws_ec2_instance) limit 3 | list kind, name
kind=aws_ec2_instance, name=sun
kind=aws_ec2_instance, name=moon
kind=aws_ec2_instance, name=star

# Same query and same result as before, with an explicit rename clause.
> query is(aws_ec2_instance) limit 3 | list kind as a, name as b
a=aws_ec2_instance, b=sun
a=aws_ec2_instance, b=moon
a=aws_ec2_instance, b=star

# Properties that do not exist, are not printed.
> query is(aws_ec2_instance) limit 3 | list kind as a, name as b, does_not_exist
a=aws_ec2_instance, b=sun
a=aws_ec2_instance, b=moon
a=aws_ec2_instance, b=star
```

### Related

- `format` - Create a string from object based on a defined format.
- `dump` - will show the complete content tree of an incoming object.
- `jq` - Define a transformation via the well known `jq` command.

## predecessors

**Select all predecessors of this node in the graph.**

```shell
predecessors [--with-origin] [edge_type]
```

This command extends an already existing query. It will select all predecessors of the currently selected nodes of the query. The graph may contain different types of edges (e.g. the `default` graph or the `delete` graph). In order to define which graph to walk, the edge_type can be specified.

If --with-origin is specified, the current element is included in the result set as well. Assume node A with descendant B with descendant C: A --> B --> C `query id(C) | predecessors` will select B, while `query id(A) | predecessors --with-origin` will select C and B.

### Options

- `--with-origin` [Optional, default to false]: includes the current element into the result set.

### Parameters

- `edge_type` [Optional, default to `default`]: Defines the type of edge to navigate.

### Environment Variables

- `edge_type` [Optional]: Defines the type of the edge to navigate. The parameter takes precedence over the env var.

### Examples

```shell
> query is(volume) and volume_status=available | predecessors | query is(volume_type)
kind=gcp_disk_type, name=pd-standard, age=2yr1mo, cloud=gcp, account=eng, region=us-central1, zone=us-central1-a
kind=gcp_disk_type, name=pd-standard, age=2yr1mo, cloud=gcp, account=sre, region=us-central1, zone=us-central1-a
kind=aws_ec2_volume_type, name=gp2, age=5d8h, cloud=aws, account=sales, region=us-west-2
```

## protect

**Mark all incoming database objects as protected.**

```shell
protect
```

Mark incoming objects as protected. All objects marked as such will not be cleaned up, even if they are marked for cleanup.

This command assumes, that all incoming elements are either objects coming from a query or are object ids. All objects coming from a query will have a property `id`. The result of this command will emit the updated object.

### Examples

```shell
# Query for instances that are tagged with "build node" - such nodes should never be clean up.
> query is(instance) and tags.job=="build node" | protect | list id, /metadata
id=ins123, protected=true

# Manually protect a list of resources.
> json ["ins123"] | protect | list id, /metadata
id=vol-123, protected=true
```

## query

**Query the graph.**

```shell
query [--with-edges] [--explain] &lt;query>
```

This command allows to query the graph using filters, traversals, functions and aggregates.

### Options

- `--with-edges`: Return edges in addition to nodes.
- `--explain`: Instead of executing the query, analyze its cost.

### Parameters

- `query` [mandatory]: The query to execute.

#### Filters

Filters have the form `path op value`.

- `path` is the complete path of names in the json structure combined with a dot (e.g. reported.cpu_count).

  In case the path contains elements, that are not json conform, they can be put into backticks (e.g. foo.bla.\`:-)\`.baz).

- `operator` is one of: `&lt;=`, `>=`, `>`, `&lt;`, `==`, `!=`, `=~`, `!~`, `in`, `not in`.

  Note: `=` is the same as `==` and `~` is the same as `=~`.

- value is a json literal (e.g. `"test"`, `23`, `[1, 2, 3]`, `true`, `{"a": 12}`).

  Note: the query parser allows to omit the parentheses for strings most of the time. In case the string contains whitespace or a special character, you should put the string into parentheses.

Example:

```shell
reported.cpu_count >= 4, name!="test", title in ["first", "second"]
```

Filters can be combined with `and` and `or` and use parentheses. Example:

```shell
(cpu_count>=4 and name!="test") or (title in ["first", "second"] and name=="test")
```

#### Traversals

Outbound traversals are traversals from a node in direction of the edge to another node, while inbound traversals walk the graph in opposite direction. Assuming 2 nodes with one connecting directed edge: `NodeA ---> NodeB`, traversing outbound from `NodeA` will yield `NodeB`, while traversing inbound from `NodeB` will yield `NodeA`.

The syntax for outbound traversals is `-->` and for inbound traversals is `&lt;--`. A traversal can be refined and allows to define the number of levels to walk in the graph:

- `-[1:1]->` (shorthand for `-->`) starts from the current node and selects all nodes that can be reached by walking exactly one step outbound.
- `-[0:1]->` starts (and includes) the current node and selects all nodes that can be reached by walking exactly one step outbound.
- `-[&lt;x>:&lt;y>]->` walks from the current node to all nodes that can be reached with x steps outbound. From here all nodes are selected including all nodes that can be reached in y steps outbound relative to the starting node.
- `-[&lt;x>]->` shorthand `-[&lt;x>:&lt;x>]->`
- `-[&lt;x>:]->` walks from the current node to all nodes that can be reached with x steps outbound. From here all nodes to the graph leafs are selected.

The same logic is used for inbound traversals (`&lt;--`, `&lt;-[0:1]-`, `&lt;-[2]-`, `&lt;-[2:]-`).

#### Functions

There are predefined functions that can be used in combination with any filter.

- is(&lt;kind>): selects all nodes that are of type &lt;kind> or any subtype of &lt;kind>. Example: is(volume) will select all GCP disks and all AWS EC2 volumes, since both types inherit from base type volume.
- id(&lt;identifier>): selects the node with the given node identifier &lt;identifier>. Example: id(foo) will select the node with id foo. The id is a synthetic id created by the collector and usually does not have a meaning, other than identifying a node uniquely.
- has_key(&lt;path>): tests if the specified name is defined in the json object. Example: is(volume) and has_key(tags, owner)

#### Aggregations

Aggregate data by using on of the following functions: `sum`, `avg`, `min`, `max` and `count`. Multiple aggregation functions can be applied to the result set by separating them by comma. Each aggregation function can be named via an optional `as &lt;name>` clause.

Aggregation functions can be grouped using aggregation values. Multiple grouping values can be defined by separating them via comma. Each grouping variable can be renamed via an optional `as &lt;name>` clause.

Examples:

```shell
> query aggregate(kind: sum(1)): is(volume)
> query aggregate(kind as kind: sum(1) as count): is(volume)
> query aggregate(kind, volume_type: sum(1) as count): is(volume)
> query aggregate(kind: sum(volume_size) as summed, sum(1) as count): is(volume)
> query aggregate(sum(volume_size) as summed, sum(1) as count): is(volume)
```

#### Sort and Limit

The number of query results can be limited to a defined number by using limit &lt;limit> and sorted by using sort &lt;sort_column> [asc, desc]. Limit and sort is allowed before a traversal and as last statement to the query result. Example: query is(volume) sort volume_size desc limit 3 &lt;-[2]- sort name limit 1

Use --explain to understand the cost of a query. A query explanation has this form (example):

```json
{
    "available_nr_items": 142670,
    "estimated_cost": 61424,
    "estimated_nr_items": 1,
    "full_collection_scan": false,
    "rating": "Simple"
}
```

- `available_nr_items` describe the number of all available nodes in the graph.
- `estimated_cost shows` the absolute cost of this query. See rating for an interpreted number.
- `estimated_nr_items` estimated number of items returned for this query. It is computed based on query statistics and heuristics and does not reflect the real number.
- `full_collection_scan` indicates, if a full collection scan is required. In case this is true, the query does not take advantage of any indexes.
- `rating` The more general rating of this query. Simple: The estimated cost is fine - the query will most probably run smoothly. Complex: The estimated cost is quite high. Check other properties. Maybe an index can be used? Bad: The estimated cost is very high. It will most probably run long and/or will take a lot of resources.

### Examples

```shell
# Query all volumes with state available
> query is(volume) and volume_status=available
kind=gcp_disk, id=71, name=gke-1, volume_status=available, age=5mo26d, cloud=gcp, account=dev, region=us-central1
kind=gcp_disk, id=12, name=pvc-2, volume_status=available, age=4mo15d, cloud=gcp, account=eng, region=us-west1
kind=gcp_disk, id=17, name=pvc-2, volume_status=available, age=9mo29d, cloud=gcp, account=eng, region=us-west1

# Other sections than reported, need to be defined from the root /
> query is(volume) and /desired.cleanup=true

# Sort and limit the number of results
> query is(volume) sort name asc limit 3
kind=aws_ec2_volume, id=vol-1, name=adf-image-1, age=2mo1d, cloud=aws, account=general-support, region=us-west-2
kind=aws_ec2_volume, id=vol-2, name=adf-image-2, age=2mo1d, cloud=aws, account=general-support, region=us-west-2

# Emit nodes together with the edges
> query --with-edges id(root) -[0:1]->
node_id=root, kind=graph_root, id=root, name=root
node_id=L_tRxI2tn6iLZdK3e8EQ3w, kind=cloud, id=gcp, name=gcp, age=5d5h, cloud=gcp
root -> L_tRxI2tn6iLZdK3e8EQ3w
node_id=WYcfqyMIkPAPoAHiEIIKOw, kind=cloud, id=aws, name=aws, age=5d5h, cloud=aws
root -> WYcfqyMIkPAPoAHiEIIKOw

# Aggregate resulting nodes
> query aggregate(kind as kind: sum(1) as count): is(volume)
group:
  kind: aws_ec2_volume
count: 1799
---
group:
  kind: gcp_disk
count: 1100

# Do not execute the query, but show an explanation of the query cost.
> query --explain is(graph_root) -[0:1]->
available_nr_items: 142670
estimated_cost: 58569
estimated_nr_items: 8
full_collection_scan: false
rating: simple
```

### Environment Variables

- `graph` [default=resoto]: the name of the graph to operate on.
- `section` [default=reported]: interpret all property paths with respect to this section. With section `reported` set, the query `name=~"test"` would be interpreted as `reported.name=~"test"`. Note: the resotoshell sets the section to reported by default. If you want to quickly override the section on one command line, you can define env vars in from of the command line (e.g.: `section=desired query clean==true`). It is possible to use absolute path using `/`, so all paths have to be defined from root (e.g.: `query desired.clean==true`)

See [https://resoto.com/docs/reference/cli/query/](https://resoto.com/docs/reference/cli/query/) for a more detailed explanation of query.

## set_desired

**Allows to set arbitrary properties as desired for all incoming database objects.**

```shell
set_desired &lt;property>=&lt;value> [&lt;property>=&lt;value> ..]
```

Set one or more desired properties for every database node that is received on the input channel. The desired state of each node in the database is merged with this new desired state, so that existing desired state not defined in this command is not touched.

This command assumes, that all incoming elements are either objects coming from a query or are object ids. All objects coming from a query will have a property `id`. The result of this command will emit the updated state.

### Parameters

- `property` - the name of the property to set in the desired section.
- `value` - the value of the property to set in the desired section. This needs to be a json element.

Multiple properties can be changed by defining multiple property=value definitions separated by space.

### Examples

```shell
> query is(instance) limit 1 | set_desired a=b b="c" num=2 | list /id, /desired
id=123, a=b, b=c, num=2

> json ["id1", "id2"] | set_desired a=b | list /id /desired
id=id1, a=b
id=id2, a=b
```

## set_metadata

**Allows to set arbitrary properties as metadata for all incoming database objects.**

```shell
set_metadata &lt;property>=&lt;value> [&lt;property>=&lt;value> ..]
```

Set one or more metadata properties for every database node that is received on the input channel. The metadata state of each node in the database is merged with this new metadata state, so that existing metadata state not defined in this command is not touched.

This command assumes, that all incoming elements are either objects coming from a query or are object ids. All objects coming from a query will have a property `id`. The result of this command will emit the updated state.

### Parameters

- `property` - the name of the property to set in the desired section.
- `value` - the value of the property to set in the desired section. This needs to be a json element.

Multiple properties can be changed by defining multiple property=value definitions separated by space.

### Examples

```shell
> query is(instance) limit 1 | set_metadata a=b b="c" num=2 | list /id, /metadata
id=123, a=b, b=c, num=2

> json ["id1", "id2"] | set_metadata a=b | list /id /metadata
id=id1, a=b
id=id2, a=b
```

## sleep

**Suspend execution for an interval of time**

```shell
sleep &lt;seconds>
```

Sleep the amount of seconds. An empty string is emitted.

### Parameters

- `seconds` the number of seconds to sleep.

#### Examples

````shell
# Print the string "6 seconds later..." after 6 seconds.
> sleep 6; echo 6 seconds later...

---
6 seconds later...


## start_task


**Start a task with the given name.**



```shell
start_task &lt;name of task>
````

Start a task with given task descriptor id.

The configured surpass behaviour of a task definition defines, if multiple tasks of the same task definition are allowed to run in parallel. In case parallel tasks are forbidden a new task can not be started. If a task could be started or not is returned as result message of this command.

### Parameters

- `task_name` [mandatory]: The name of the related task definition.

### Examples

```shell
> start_task example_task
```

See: add_job, delete_job, jobs

## successors

**Select all successor of this node in the graph.**

```shell
successors [--with-origin] [edge_type]
```

This command extends an already existing query. It will select all successors of the currently selected nodes of the query. The graph may contain different types of edges (e.g. the `default` graph or the `delete` graph). In order to define which graph to walk, the edge_type can be specified.

If --with-origin is specified, the current element is included in the result set as well. Assume node A with descendant B with descendant C: A --> B --> C `query id(A) | successors` will select B, while `query id(A) | successors --with-origin` will select C and B.

### Options

- `--with-origin` [Optional, default to false]: includes the current element into the result set.

### Parameters

- `edge_type` [Optional, default to `default`]: Defines the type of edge to navigate.

### Environment Variables

- `edge_type` [Optional]: Defines the type of the edge to navigate. The parameter takes precedence over the env var.

### Examples

```shell
> query is(volume_type) | successors | query is(volume)
kind=gcp_disk, id=16, name=gke16, age=8mo29d, cloud=gcp, account=eng, region=us-west1, zone=us-west1-a
kind=gcp_disk, id=26, name=gke26, age=8mo29d, cloud=gcp, account=eng, region=us-west1, zone=us-west1-a
kind=aws_ec2_volume, id=vol1, name=vol1, age=2mo11d, cloud=aws, account=insights, region=us-west-2
```

## system

**Access and manage system wide properties.**

```
system backup create [name]
system backup restore &lt;path>
system info
```

### Parameters

- `name` [optional] - the file name of the backup that is created. If no name is provided, a new name is created by using the current time and this format: `backup_yyyyMMdd_hmm`. Example: backup_20211022_1028
- `path` [mandatory] - path to the local backup file.

### Backup creation

Create a system backup for the complete database, which contains:

- backup of all graph data
- backup of all model data
- backup of all persisted jobs/tasks data
- backup of all subscribers data
- backup of all configuration data

This backup can be restored via `system backup restore &lt;path>`. Since this command creates a complete backup, it can be restored to an empty database.

_Note_: a backup acquires a global write lock. This basically means, that _no write_ can be performed, while the backup is created! The backup is not encrypted.

### Restore a backup

The complete database state from a previously generated backup can be restored. All existing data in the database will be overwritten. This command will not wipe any existing data: if there are collections in the database, that are not included in the backup, it will not be deleted by this process. In order to restore exactly the same state as in the backup, you should start from an empty database.

_Note_: a backup acquires a global write lock. This basically means, that _no write_ can be performed, while the backup is restored! After the restore process is done, the resotocore process will stop. It should be restarted by the process supervisor automatically. The restart is necessary to take effect from the changed underlying data source.

### System information

Prints information about the currently running system.

### Examples

```shell
# Create a backup. The name of the backup will have the current time.
> system backup create
Received a file backup_20220202_1121, which is stored to ./backup_20220202_1121.

# Create a backup and provide a name for it
> system backup create bck_1234
Received a file bck_1234, which is stored to ./bck_1234.

# Restore a backup. This will stop the running resotocore instance.
> system backup restore bck_1234
Database has been restored successfully!
Since all data has changed in the database eventually, this service needs to be restarted!

# Show system information.
> system info
name: resotocore
version: 2.0.0a14
cpus: 8
mem_available: 2.85 GiB
mem_total: 16.00 GiB
inside_docker: false
started_at: '2022-02-02T11:23:19Z'
```

## tag

**Update a tag with provided value or delete a tag**

```
tag update [--nowait] [tag_name new_value]
tag delete [--nowait] [tag_name]
```

This command can be used to update or delete a specific tag. Tags have a name and value - both name and value are strings.

When this command is issued, the change is done on the cloud resource via the cloud specific provider. The change in the graph data itself is reflected with this operation. In rare case it might take up to the next collect run.

When a tag is updated, the new value can be defined as static string or as format string using curly braces. All placeholders in a format string are replaced with values from the related resource (see `format` for details).

After the tag of a resource is updated or deleted the resulting data is provided as output of this command and can be used for further chained operations.

The command would wait for the worker to report the result back synchronously. Once the cli command returns, also the tag update/delete is finished. If the command should not wait for the result, the action can be performed in background via the `--nowait` flag.

The input of this command is either a query result or the identifier of the resource as string.

### Options

- `--nowait` if this flag is defined, the cli will send the tag command to the worker and will not wait for the task to finish.

### Parameters

- `tag_name` [mandatory]: the name of the tag to change
- `tag_value` [mandatory]: in case of update: the new value of the tag*name. The tag_value can use format templates (`help format`) to define the value with backreferences from the object. Example: test*{name}\_{kind} -> test_pvc-123_disk

### Examples

```shell
# Make sure there is no resource that is tagged with 'foo'
> query is(resource) and tags.foo!=null | tag delete foo
kind=aws_ec2_keypair, id=key-0, name=default, age=1yr8mo, cloud=aws, account=eng-sre, region=us-west-2

# Manually select the resources to tag by using the id.
> json["key-0"] | tag delete foo
kind=aws_ec2_keypair, id=key-0, name=default, age=1yr8mo, cloud=aws, account=eng-sre, region=us-west-2

# Updating a tag by using a format template.
> query is(volume) and tags.owner == null limit 1 | tag update owner "gen_{/ancestors.account.reported.name}_{name}"
kind=gcp_disk, id=123, name=gke-1, age=5mo27d, cloud=gcp, account=eng, region=us-central1, zone=us-central1-c
```

## tail

**Return n last elements of the stream.**

```shell
tail [-num]
```

Take the last [num] number of elements from the input stream and send them downstream. The beginning of the stream is consumed and discarded.

Note: using a query, the same result can be achieved using `sort` and `limit`.

### Options

- `-num` [optional, defaults to 100]: the number of elements to take from the head.

### Examples

```shell
# Json array with 5 elements is defined. We only take the last 2 elements.
> json [1,2,3,4,5] | tail -2
4
5

# A query is performed to select all volumes. Only the last 2 results are taken.
> query is(volume) | tail -2
kind=aws_ec2_volume, id=vol-0, name=vol-0, age=2mo1d, cloud=aws, account=dev, region=us-west-2
kind=gcp_disk, id=123, name=gke-1, age=7mo22d, cloud=gcp, account=eng, region=us-west1, zone=us-west1-a
```

### Related

- `head` - take a defined number of elements.

## templates

**Access the query template library.**

```shell
templates
templates &lt;name_of_template>
templates add &lt;name_of_template> &lt;query_template>
templates update &lt;name_of_template> &lt;query_template>
templates delete &lt;name_of_template>
templates test key1=value1, key2=value2, ..., keyN=valueN &lt;template_to_expand>
```

- `templates: get the list of all templates
- `templates &lt;name>`: get the current definition of the template defined by given template name
- `templates add &lt;name> &lt;template>`: add a query template to the query template library under given name.
- `templates update &lt;name> &lt;template>`: update a query template in the query template library.
- `templates delete &lt;name>`: delete the query template with given name.
- `templates test k=v &lt;template_to_expand>`: test the defined template.

Placeholders are defined in 2 double curly braces {{placeholder}} and get replaced by the provided placeholder value during render_console time. The name of the placeholder can be any valid alphanumeric string. The template 'is({{kind}})' with expand parameters kind=volume becomes 'is(volume)' during expand time.

### Parameters

- `name_of_template`: The name of the query template.
- `query_template`: The query with template placeholders.
- `key=value`: any number of key/value pairs separated by comma

### Examples

```shell
# Test a template by populating it with provided key/value pairs
> templates test kind=volume is({{kind}})
is(volume)

# Add a very simple template with name filter_kind to the query library
> templates add filter_kind is({{kind}})
Template filter_kind added to the query library.
is({{kind}})

# List all templates in the query library
> templates
filter_kind: is({{kind}})

# Show one specific template by provided name
> templates filter_kind
is({{kind}})

# Use this template in a query
> query expand(filter_kind, kind=volume) and name=~dkl
kind=aws_ec2_volume, id=vol-1, name=dkl-3, age=2mo2d, cloud=aws, account=eng, region=us-west-2

> templates delete filter_kind
Template filter_kind deleted from the query library.
```

## uniq

**Remove all duplicated objects from the stream.**

```shell
uniq
```

All elements flowing through the uniq command are analyzed and all duplicates get removed. Note: a hash value is computed from json objects, which is ignorant of the order of properties, so that `{"a": 1, "b": 2}` is declared equal to `{"b": 2, "a": 1}`

### Examples

```shell
# Multiple occurrences of the same element are sorted out.
> json [1, 2, 3, 1, 2, 3] | uniq
1
2
3

# The same logic applies to json objects
> json [{"a": 1, "b": 2}, {"b": 2, "a": 1}] | uniq
a: 1
b: 2
```

## write

**Writes the incoming stream of data to a file in the defined format.**

```shell
write &lt;file-name>
```

Writes the result of this command to a file with given name.

### Parameters

- `file-name` [mandatory]: The name of the file to write to.

### Examples

```shell
# Select 3 resources, format them as json and write it to the file out.json.
> query all limit 3 | format --json | write out.json
Received a file out.json, which is stored to ./out.json.

# Select the root node and traverse 2 levels deep. Format the result as dot graph and write it to out.dot.
> query --with-edges id(root) -[0:2]-> | format --dot | write out.dot
Received a file out.dot, which is stored to ./out.dot.
```

## Command Aliases

| Alias            | Command      | Description                             |
| ---------------- | ------------ | --------------------------------------- |
| `https`          | `http`       | Perform http request with incoming data |
| `match`          | `query`      | Query the graph.                        |
| `start_workflow` | `start_task` | Start a task with the given name.       |

## Placeholder Strings

| Placeholder   | Example                |
| ------------- | ---------------------- |
| `@DAY@`       | `16`                   |
| `@FRIDAY@`    | `2022-02-18`           |
| `@HOUR@`      | `15`                   |
| `@MINUTE@`    | `23`                   |
| `@MONDAY@`    | `2022-02-21`           |
| `@MONTH@`     | `02`                   |
| `@NOW@`       | `2022-02-16T15:23:31Z` |
| `@SATURDAY@`  | `2022-02-19`           |
| `@SECOND@`    | `31`                   |
| `@SUNDAY@`    | `2022-02-20`           |
| `@THURSDAY@`  | `2022-02-17`           |
| `@TIME@`      | `15:23:31`             |
| `@TODAY@`     | `2022-02-16`           |
| `@TOMORROW@`  | `2022-02-17`           |
| `@TUESDAY@`   | `2022-02-22`           |
| `@TZ@`        | `CET`                  |
| `@TZ_OFFSET@` | `+0100`                |
| `@UTC@`       | `2022-02-16T14:23:31Z` |
| `@WEDNESDAY@` | `2022-02-16`           |
| `@YEAR@`      | `2022`                 |
| `@YESTERDAY@` | `2022-02-15`           |
